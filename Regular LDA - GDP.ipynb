{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ms5941/.local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "\n",
    "import gensim\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from gensim.models import ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEME = 'GDP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-25 23:32:55,513 : INFO : loading Dictionary object from /work/ms5941/NLP/Temp/GDP/GDP_less_restricted.dict\n",
      "2021-04-25 23:32:55,519 : INFO : Dictionary lifecycle event {'fname': '/work/ms5941/NLP/Temp/GDP/GDP_less_restricted.dict', 'datetime': '2021-04-25T23:32:55.518595', 'gensim': '4.0.1', 'python': '3.7.7 (default, May  7 2020, 21:25:33) \\n[GCC 7.3.0]', 'platform': 'Linux-4.19.0-16-amd64-x86_64-with-debian-10.9', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Load dictionary and corpus\n",
    "dictionary_all = gensim.corpora.Dictionary.load(TEMP_PATH + '/%s/%s_less_restricted.dict' % (THEME, THEME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Look for ids of related words to theme in the dictionary. This will be a list of ids for each year.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(TEMP_PATH + '/%s_words.csv' % THEME.lower(), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_name_list = dict()\n",
    "year_id_list = dict()\n",
    "for col in words.columns:\n",
    "    reqd_words = list(words[col].values) + [THEME.lower()]\n",
    "    reqd_ids = [k for k,v in dictionary_all.items() if v in reqd_words]\n",
    "    year_name_list[col] = [dictionary_all[i] for i in reqd_ids]\n",
    "    year_id_list[col] = reqd_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1996': ['inflation',\n",
       "  'figure',\n",
       "  'gdp',\n",
       "  'economists',\n",
       "  'unemployment',\n",
       "  'gross',\n",
       "  'cpi'],\n",
       " '1997': ['consumption',\n",
       "  'deficit',\n",
       "  'inflation',\n",
       "  'gdp',\n",
       "  'unemployment',\n",
       "  'gross',\n",
       "  'cpi'],\n",
       " '1998': ['consumption',\n",
       "  'deficit',\n",
       "  'inflation',\n",
       "  'gdp',\n",
       "  'employment',\n",
       "  'unemployment',\n",
       "  'cpi',\n",
       "  'eci'],\n",
       " '1999': ['deficit',\n",
       "  'inflation',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'employment',\n",
       "  'unemployment',\n",
       "  'cpi'],\n",
       " '2000': ['inflation',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'revised',\n",
       "  'economists',\n",
       "  'employment',\n",
       "  'cpi',\n",
       "  'pmi'],\n",
       " '2001': ['consumption',\n",
       "  'inflation',\n",
       "  'figure',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'revised',\n",
       "  'employment',\n",
       "  'gross',\n",
       "  'reading',\n",
       "  'cpi'],\n",
       " '2002': ['consumption',\n",
       "  'inflation',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'employment',\n",
       "  'gross',\n",
       "  'reading',\n",
       "  'cpi'],\n",
       " '2003': ['inflation', 'gdp', 'payrolls', 'jobless'],\n",
       " '2004': ['inflation', 'output', 'gdp', 'payrolls', 'jobs', 'cpi', 'pmi'],\n",
       " '2005': ['inflation',\n",
       "  'output',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'jobless',\n",
       "  'factory',\n",
       "  'cpi'],\n",
       " '2006': ['deficit',\n",
       "  'inflation',\n",
       "  'output',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'revised',\n",
       "  'jobs',\n",
       "  'cpi'],\n",
       " '2007': ['deficit', 'inflation', 'gdp', 'payrolls', 'cpi'],\n",
       " '2008': ['deficit', 'inflation', 'gdp', 'payrolls', 'cpi'],\n",
       " '2009': ['deficit', 'exports', 'inflation', 'gdp', 'payrolls', 'cpi'],\n",
       " '2010': ['consumption',\n",
       "  'deficit',\n",
       "  'inflation',\n",
       "  'output',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'cpi',\n",
       "  'pmi'],\n",
       " '2011': ['consumption',\n",
       "  'inflation',\n",
       "  'output',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'employment',\n",
       "  'reading',\n",
       "  'cpi',\n",
       "  'pmi'],\n",
       " '2012': ['deficit', 'inflation', 'gdp', 'payrolls', 'cpi', 'pmi', 'growth'],\n",
       " '2013': ['deficit',\n",
       "  'inflation',\n",
       "  'output',\n",
       "  'gdp',\n",
       "  'payrolls',\n",
       "  'reading',\n",
       "  'cpi',\n",
       "  'pmi',\n",
       "  'growth'],\n",
       " '2014': ['deficit', 'inflation', 'gdp', 'jobs', 'cpi', 'pmi', 'growth'],\n",
       " '2015': ['deficit', 'inflation', 'gdp', 'factory', 'cpi', 'pmi', 'growth'],\n",
       " '2016': ['manufacturing',\n",
       "  'inflation',\n",
       "  'gdp',\n",
       "  'jobless',\n",
       "  'factory',\n",
       "  'cpi',\n",
       "  'pmi',\n",
       "  'growth'],\n",
       " '2017': ['inflation', 'gdp', 'cpi', 'pmi'],\n",
       " '2018': ['deficit', 'inflation', 'gdp', 'cpi', 'pmi'],\n",
       " '2019': ['deficit', 'inflation', 'gdp', 'cpi', 'surplus', 'pmi'],\n",
       " '2020': ['deficit', 'inflation', 'gdp', 'cpi', 'surplus', 'pmi']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(START_YEAR, END_YEAR + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1996.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1997.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1998.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1999.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2000.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2001.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2002.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2003.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2004.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2005.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2006.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2007.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2008.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2009.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2010.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2011.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2012.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2013.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2014.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2015.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2016.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2017.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2018.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2019.json done!\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2020.json done!\n"
     ]
    }
   ],
   "source": [
    "good_words_count_per_year = dict()\n",
    "for year in years:\n",
    "    good_words_count_per_year[year] = []\n",
    "    with open(TOKENIZED_ARTICLES_PATH % (THEME, THEME, year)) as f:\n",
    "        a = json.load(f)\n",
    "        for article in a:\n",
    "            good_words_count_per_year[year].append(len([word for word in article if word in year_name_list[year]]))\n",
    "    print(TOKENIZED_ARTICLES_PATH % (THEME, THEME, year), 'done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1996.json done!\n",
      "Count for the year: 313 Vs original: 30085\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1997.json done!\n",
      "Count for the year: 235 Vs original: 21451\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1998.json done!\n",
      "Count for the year: 1403 Vs original: 121502\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_1999.json done!\n",
      "Count for the year: 890 Vs original: 67258\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2000.json done!\n",
      "Count for the year: 602 Vs original: 53084\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2001.json done!\n",
      "Count for the year: 691 Vs original: 57614\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2002.json done!\n",
      "Count for the year: 555 Vs original: 54160\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2003.json done!\n",
      "Count for the year: 1028 Vs original: 102461\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2004.json done!\n",
      "Count for the year: 1121 Vs original: 91690\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2005.json done!\n",
      "Count for the year: 1071 Vs original: 97347\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2006.json done!\n",
      "Count for the year: 1720 Vs original: 171795\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2007.json done!\n",
      "Count for the year: 2276 Vs original: 185017\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2008.json done!\n",
      "Count for the year: 2812 Vs original: 219047\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2009.json done!\n",
      "Count for the year: 2574 Vs original: 200514\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2010.json done!\n",
      "Count for the year: 1273 Vs original: 102891\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2011.json done!\n",
      "Count for the year: 1296 Vs original: 117472\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2012.json done!\n",
      "Count for the year: 1032 Vs original: 97383\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2013.json done!\n",
      "Count for the year: 1264 Vs original: 113546\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2014.json done!\n",
      "Count for the year: 1737 Vs original: 133759\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2015.json done!\n",
      "Count for the year: 1722 Vs original: 167170\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2016.json done!\n",
      "Count for the year: 2401 Vs original: 210787\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2017.json done!\n",
      "Count for the year: 1921 Vs original: 192112\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2018.json done!\n",
      "Count for the year: 2852 Vs original: 206492\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2019.json done!\n",
      "Count for the year: 3524 Vs original: 204138\n",
      "/work/ms5941/NLP/Temp/GDP/GDP_Articles_Tokenized_2020.json done!\n",
      "Count for the year: 623 Vs original: 53819\n"
     ]
    }
   ],
   "source": [
    "theme = 'GDP'\n",
    "\n",
    "yearly_slices = []\n",
    "for year in years:\n",
    "    # Get the 1% threshold count (the top 1% of most relevant articles)\n",
    "    x = good_words_count_per_year[year]\n",
    "    x.sort()\n",
    "    threshold = x[-int(len(x)*0.01):][0]\n",
    "    year_article_count = 0\n",
    "    with open(TOKENIZED_ARTICLES_PATH % (theme, theme, year)) as f:\n",
    "        article_index = 0\n",
    "        all_articles = json.load(f)\n",
    "        for articles in all_articles:\n",
    "            if good_words_count_per_year[year][article_index] >= threshold:\n",
    "                year_article_count += 1    \n",
    "            article_index += 1\n",
    "    yearly_slices.append(year_article_count)\n",
    "    print(TOKENIZED_ARTICLES_PATH % (theme, theme, year), 'done!')\n",
    "    print('Count for the year:', year_article_count, 'Vs original:', len(all_articles))\n",
    "\n",
    "# dictionary_all = gensim.corpora.Dictionary.load(TEMP_PATH + '/%s/%s_less_restricted.dict' % (theme, theme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(yearly_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEMP_PATH + '/%s/%s_yearly_slices.txt' % (THEME, THEME), 'w') as f:\n",
    "    json.dump(yearly_slices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEMP_PATH + '/%s/%s_yearly_slices.txt' % (THEME, THEME), 'r') as f:\n",
    "    yearly_slices = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[313, 235, 1403, 890, 602, 691, 555, 1028, 1121, 1071, 1720, 2276, 2812, 2574, 1273, 1296, 1032, 1264, 1737, 1722, 2401, 1921, 2852, 3524, 623]\n"
     ]
    }
   ],
   "source": [
    "print(yearly_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-26 00:19:18,180 : INFO : loading Dictionary object from /work/ms5941/NLP/Temp/GDP/GDP_less_restricted.dict\n",
      "2021-04-26 00:19:18,185 : INFO : Dictionary lifecycle event {'fname': '/work/ms5941/NLP/Temp/GDP/GDP_less_restricted.dict', 'datetime': '2021-04-26T00:19:18.184290', 'gensim': '4.0.1', 'python': '3.7.7 (default, May  7 2020, 21:25:33) \\n[GCC 7.3.0]', 'platform': 'Linux-4.19.0-16-amd64-x86_64-with-debian-10.9', 'event': 'loaded'}\n",
      "2021-04-26 00:19:18,203 : INFO : loaded corpus index from /work/ms5941/NLP/Temp/GDP/GDP_less_restricted.mm.index\n",
      "2021-04-26 00:19:18,205 : INFO : initializing cython corpus reader from /work/ms5941/NLP/Temp/GDP/GDP_less_restricted.mm\n",
      "2021-04-26 00:19:18,225 : INFO : accepted corpus with 36936 documents, 1662 features, 4916481 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "dictionary_all = gensim.corpora.Dictionary.load(TEMP_PATH + '/%s/%s_less_restricted.dict' % (THEME, THEME))\n",
    "corpus_all = gensim.corpora.MmCorpus(TEMP_PATH + '/%s/%s_less_restricted.mm' % (THEME, THEME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cum_yearly_slices = np.cumsum(yearly_slices)\n",
    "\n",
    "corpus_by_year = dict()\n",
    "corpus_by_year[START_YEAR] = corpus_all[:cum_yearly_slices[0]]\n",
    "for i in range(1, 25):\n",
    "    corpus_by_year[START_YEAR + i] = corpus_all[cum_yearly_slices[i-1]:cum_yearly_slices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2574"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_by_year[2009]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_avg_topic_probabilities(lda, corp, num_topics):\n",
    "    \"\"\"\n",
    "    For the given LDA model and corpus, get the aggregate probability of each topic \n",
    "    (by iterating over each document in the corpus, adding up individual probabilities and aggregating)\n",
    "    Then, divide by the total number of documents in the corpus to get the average \n",
    "    topic probabilities for the corpus.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_topics_probabilities = np.zeros(num_topics)\n",
    "    for article in corp:\n",
    "        article_topics = lda.get_document_topics(article)\n",
    "        topic_vec = np.zeros(num_topics)\n",
    "        for k, prob in article_topics:\n",
    "            topic_vec[k] = prob\n",
    "        all_topics_probabilities += topic_vec\n",
    "    \n",
    "    # Avg topic probabilities\n",
    "    avg_topic_probabilities = all_topics_probabilities/float(len(corp))\n",
    "    \n",
    "    return avg_topic_probabilities\n",
    "\n",
    "\n",
    "def get_top_ten_topics_for_year(year, lda, avg_topic_probabilities):\n",
    "    \"\"\"\n",
    "    Using the average topic probabilites, rank the topics and \n",
    "    return the top ten topics for a year.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get top 10 topics for each year\n",
    "    indices = (-avg_topic_probabilities).argsort()[:10]\n",
    "\n",
    "    top_topics_words = dict()\n",
    "    top_topics_words[year] = dict()\n",
    "    rank = 1\n",
    "    for ind in indices:\n",
    "        top_words = lda.show_topic(ind, topn=10)\n",
    "        words, probs = zip(*top_words)\n",
    "        top_topics_words[year][rank] = top_words\n",
    "        rank += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict({(i,j): [x[0] for x in top_topics_words[i][j]] for i in top_topics_words.keys() \n",
    "                            for j in top_topics_words[i].keys()}).T\n",
    "    return df, top_topics_words\n",
    "\n",
    "\n",
    "\n",
    "def get_largest_popularity_shifts(avg_topics_all, num_topics=50, passes=50, iterations=400, \n",
    "                                  eval_every=None, update_every=0, alpha='auto', eta='auto'):\n",
    "\n",
    "    # Get topic ranks for each year\n",
    "    yearly_ranks = dict()\n",
    "\n",
    "    temp = (-avg_topics_all[0]).argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(avg_topics_all[0]))\n",
    "    yearly_ranks[2019] = ranks\n",
    "\n",
    "    temp = (-avg_topics_all[1]).argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(avg_topics_all[1]))\n",
    "    yearly_ranks[2020] = ranks\n",
    "    \n",
    "    shift_in_popularity = yearly_ranks[2019] - yearly_ranks[2020]\n",
    "    \n",
    "    top_shifts = dict()\n",
    "    # Top gains indices\n",
    "    top_shifts['upward'] = (shift_in_popularity).argsort()[:5]\n",
    "    \n",
    "    # Top drops indices\n",
    "    top_shifts['downward'] = (-shift_in_popularity).argsort()[:5]\n",
    "\n",
    "    for trend in ['upward', 'downward']:\n",
    "        print('Top %s shifts in popularity (2019 to 2020)' % trend)\n",
    "        top_shift_topics_words = dict()\n",
    "        rank = 1\n",
    "        for ind in top_shifts[trend]:\n",
    "            tempfile = datapath('model_'+ '_'.join([str(num_topics), str(passes), str(iterations), str(alpha), str(eta)]))\n",
    "            if os.path.exists(tempfile):\n",
    "                lda = gensim.models.LdaModel.load(tempfile)    \n",
    "            top_words = lda.show_topic(ind, topn=12)\n",
    "            words, probs = zip(*top_words)\n",
    "            top_shift_topics_words[rank] = top_words\n",
    "            rank += 1\n",
    "\n",
    "        display(pd.DataFrame.from_dict({x:[y[0] for y in top_shift_topics_words[x]] for x in top_shift_topics_words}).T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lda_model(corpus, dictionary, num_topics=10, passes=25, \n",
    "                       iterations=400, eval_every=None, update_every=0, \n",
    "                       alpha='auto', eta='auto'):\n",
    "\n",
    "    lda = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, alpha='auto', eta='auto',\n",
    "                                 iterations=iterations, num_topics=num_topics, passes=passes, \n",
    "                                 eval_every=eval_every, update_every = update_every)\n",
    "    \n",
    "    # Save lda model\n",
    "    tempfile = TEMP_PATH + '/%s/%s_LDA_model_' % (THEME, THEME) + '_'.join([str(num_topics), str(passes), str(iterations), str(alpha), str(eta)]) \n",
    "    lda.save(tempfile)\n",
    "    \n",
    "    return lda\n",
    "\n",
    "\n",
    "\n",
    "def get_topics(corpus_all, dictionary_all, corpus_by_year, num_topics=10, passes=25, iterations=400, \n",
    "               eval_every=None, update_every=0, alpha='auto', eta='auto'):\n",
    "    \"\"\"\n",
    "    Get the top topics for each year, based on an LDA model created using documents of both years\n",
    "    \n",
    "    \"\"\"\n",
    "    # Check if a model with the same config already exists. \n",
    "    # If it does, load the model instead of generating a new one\n",
    "    tempfile = TEMP_PATH + '/%s/%s_LDA_model_' % (THEME, THEME) + '_'.join([str(num_topics), str(passes), str(iterations), str(alpha), str(eta)]) \n",
    "    if os.path.exists(tempfile):\n",
    "        lda = gensim.models.LdaModel.load(tempfile)\n",
    "    else:\n",
    "        lda = generate_lda_model(corpus_all, dictionary_all, num_topics, passes, \n",
    "                                 iterations, eval_every, update_every, alpha, eta)\n",
    "\n",
    "    avg_topics_all = []\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        avg_topic_probabilities = get_avg_topic_probabilities(lda, corpus_by_year[year], num_topics)\n",
    "        df, top_topic_words = get_top_ten_topics_for_year(year, lda, avg_topic_probabilities)\n",
    "        display(df)\n",
    "        avg_topics_all.append(avg_topic_probabilities)\n",
    "\n",
    "    # Plot avg topic distribution graphs\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "    ax[0].bar(range(len(avg_topics_all[0])), avg_topics_all[0]*100)\n",
    "    ax[1].bar(range(len(avg_topics_all[1])), avg_topics_all[1]*100)\n",
    "    plt.setp(ax, ylim=(0, 20))\n",
    "    plt.show()\n",
    "\n",
    "    return avg_topics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-26 02:03:06,537 : INFO : using autotuned alpha, starting with [0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667]\n",
      "2021-04-26 02:03:06,538 : INFO : using serial LDA version on this node\n",
      "2021-04-26 02:03:06,541 : INFO : running batch LDA training, 15 topics, 25 passes over the supplied corpus of 36936 documents, updating model once every 36936 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2021-04-26 02:03:06,611 : INFO : PROGRESS: pass 0, at document #2000/36936\n",
      "2021-04-26 02:03:12,185 : INFO : optimized alpha [0.049309283, 0.051573273, 0.046643257, 0.050913095, 0.051524978, 0.05293616, 0.046389274, 0.04836704, 0.047335636, 0.051053293, 0.060594134, 0.04946222, 0.04998807, 0.049203314, 0.049886327]\n",
      "2021-04-26 02:03:12,280 : INFO : PROGRESS: pass 0, at document #4000/36936\n",
      "2021-04-26 02:03:19,123 : INFO : optimized alpha [0.04148083, 0.04174305, 0.036303844, 0.04140546, 0.043570686, 0.043178394, 0.036116164, 0.03863625, 0.036486298, 0.042380467, 0.053995036, 0.040607687, 0.040863812, 0.04030327, 0.041148063]\n",
      "2021-04-26 02:03:19,218 : INFO : PROGRESS: pass 0, at document #6000/36936\n",
      "2021-04-26 02:03:26,357 : INFO : optimized alpha [0.03438183, 0.03230982, 0.03326261, 0.035482027, 0.039086573, 0.039591085, 0.028390396, 0.030138986, 0.02916557, 0.035962395, 0.055544488, 0.032190587, 0.03572397, 0.0340559, 0.034933805]\n",
      "2021-04-26 02:03:26,459 : INFO : PROGRESS: pass 0, at document #8000/36936\n",
      "2021-04-26 02:03:33,368 : INFO : optimized alpha [0.029101217, 0.027961314, 0.028121376, 0.03124727, 0.035197027, 0.03466492, 0.023543347, 0.025573181, 0.024480853, 0.031131115, 0.055452332, 0.026861781, 0.032726586, 0.029290445, 0.030221546]\n",
      "2021-04-26 02:03:33,467 : INFO : PROGRESS: pass 0, at document #10000/36936\n",
      "2021-04-26 02:03:45,625 : INFO : optimized alpha [0.025495425, 0.023312753, 0.02913051, 0.027776547, 0.032385014, 0.031332273, 0.020578025, 0.021771537, 0.021695191, 0.025698023, 0.047303364, 0.023076767, 0.031009581, 0.0256785, 0.027963739]\n",
      "2021-04-26 02:03:45,843 : INFO : PROGRESS: pass 0, at document #12000/36936\n",
      "2021-04-26 02:04:00,229 : INFO : optimized alpha [0.021620827, 0.020248879, 0.031276148, 0.02497685, 0.03019717, 0.028110612, 0.017931107, 0.019448528, 0.019864671, 0.022827066, 0.04127114, 0.02032406, 0.030065145, 0.02381356, 0.024406139]\n",
      "2021-04-26 02:04:00,449 : INFO : PROGRESS: pass 0, at document #14000/36936\n",
      "2021-04-26 02:04:16,316 : INFO : optimized alpha [0.018888274, 0.02039564, 0.027644675, 0.022324942, 0.02722569, 0.025813878, 0.01576578, 0.016915109, 0.018354995, 0.021691704, 0.042366736, 0.018386917, 0.027050288, 0.021696895, 0.024577115]\n",
      "2021-04-26 02:04:16,559 : INFO : PROGRESS: pass 0, at document #16000/36936\n",
      "2021-04-26 02:04:32,502 : INFO : optimized alpha [0.01671679, 0.01854852, 0.025463805, 0.021071903, 0.02739273, 0.024173858, 0.014556849, 0.015216846, 0.016632415, 0.020060923, 0.037991136, 0.016403738, 0.03144913, 0.019220937, 0.022150801]\n",
      "2021-04-26 02:04:32,739 : INFO : PROGRESS: pass 0, at document #18000/36936\n",
      "2021-04-26 02:04:48,846 : INFO : optimized alpha [0.015127374, 0.016933661, 0.022675015, 0.020729333, 0.029151298, 0.02210202, 0.0136084985, 0.013621726, 0.014758163, 0.019309737, 0.031224387, 0.014786592, 0.037534658, 0.017558748, 0.02004371]\n",
      "2021-04-26 02:04:49,077 : INFO : PROGRESS: pass 0, at document #20000/36936\n",
      "2021-04-26 02:05:05,111 : INFO : optimized alpha [0.013999948, 0.0155867925, 0.019867266, 0.020844141, 0.027285993, 0.023681182, 0.012519661, 0.012552376, 0.01349997, 0.018659888, 0.025998581, 0.013986102, 0.03952389, 0.016227148, 0.018196663]\n",
      "2021-04-26 02:05:05,400 : INFO : PROGRESS: pass 0, at document #22000/36936\n"
     ]
    }
   ],
   "source": [
    "avg_topics_all = get_topics(corpus_all, dictionary_all, corpus_by_year, num_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
